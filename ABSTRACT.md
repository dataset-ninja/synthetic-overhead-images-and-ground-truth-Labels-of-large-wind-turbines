The authors of the dataset present a collection of synthetic overhead imagery of wind turbines, which was generated using CityEngine. For each image, corresponding labels are provided in the YOLOv3 format, containing essential information such as the class, x and y coordinates, and height and width of the ground truth bounding boxes for each wind turbine.

The motivation behind creating this dataset was to address the challenge of acquiring sufficient data for training object detection models on wind turbines. Since wind turbines are rare and sparse, gathering data can be costly. The authors propose using synthetic imagery to automate the generation of new training data, thereby mitigating the scarcity of real-world samples. Furthermore, the use of synthetic imagery can help with cross-domain testing, where the model's performance may suffer due to limited training data from certain regions.

To generate the dataset, the authors selected background images from [Power Plant Satellite Imagery Dataset](https://figshare.com/articles/dataset/Power_Plant_Satellite_Imagery_Dataset/5307364), ensuring that they were not part of the existing "Overhead Imagery of Wind Turbines" dataset and lacked excessive infrastructure that might render the scene unrealistic. Subsequently, a script was employed to randomly and uniformly generate 3D models of large wind turbines over the selected background images, and the virtual camera was positioned to save four 608x608 pixel images. This process was repeated with a consistent random seed, but this time the background image was omitted, and the wind turbines were colored in black. The resulting black and white images were then converted into ground truth labels by grouping the black pixels to represent the wind turbines' locations.